{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "This notebook compares the original NN and the PSO-NN for different datasets.\n",
    "\n",
    "The dataset can be used via libraries like `sklearn` or `tensorflow`. The datasets used are:\n",
    "- Iris\n",
    "- Wine\n",
    "- Breast Cancer\n",
    "\n",
    "Comparing the performance of PSO-based optimization with traditional backpropagation\n",
    "in terms of accuracy and convergence speed.\n",
    "✓ Comparing behaviour and results in three different datasets your choice. You are\n",
    "allowed to choose any, but you are expected to reason, explain and compare the results.\n",
    "✓ Analyzing the influence of various PSO parameters (w, c1, c2, and velocity limits) on\n",
    "optimization performance.\n",
    "✓ Identifying strengths and weaknesses of PSO for neural network training, particularly in\n",
    "terms of handling high-dimensional search spaces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['x_train', 'x_test', 'y_train', 'y_test'])\n"
     ]
    }
   ],
   "source": [
    "#Loading all 3 dataset and creating train test splits.\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "#import iris dataset from sklearn\n",
    "from sklearn.datasets import load_iris,load_breast_cancer,load_wine\n",
    "\n",
    "\n",
    "#Loading the dataset from the internet\n",
    "datasets = [load_iris(),load_breast_cancer(),load_wine()]\n",
    "dataset_names = ['iris','breast_cancer','wine']\n",
    "#create train test splits for each dataset and put them in a dictionary\n",
    "sets = []\n",
    "for i,dataset in enumerate(datasets):\n",
    "    datadict ={}\n",
    "    x_train,x_test,y_train,y_test = train_test_split(dataset.data,dataset.target,test_size=0.2,random_state=42)\n",
    "    datadict['x_train'] = x_train\n",
    "    datadict['x_test'] = x_test\n",
    "    datadict['y_train'] = y_train\n",
    "    datadict['y_test'] = y_test\n",
    "    sets.append(datadict)\n",
    "    \n",
    "print(sets[0].keys())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perofrmance Comparison\n",
    "\n",
    "Comparing the performance of PSO-based optimization with traditional backpropagation\n",
    "in terms of accuracy and convergence speed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "############ You are using the following settings:\n",
      "Number of hidden layers: 10\n",
      "Activation function: <function sigmoid at 0x0000022B82AC4A60>\n",
      "Number of variables to optimize: 332\n",
      "PSO parameters C1: 2.0 C2: 2.0 W: 0.7 SwarmSize: 100 Iterations: 1000\n",
      "\n",
      "\n",
      "PSO optimization time: 11.37 seconds\n",
      "Accuracy PSO-NN: 0.99\n",
      "Epoch 0, Loss: 0.6947\n",
      "Epoch 100, Loss: 0.6599\n",
      "Epoch 200, Loss: 0.6597\n",
      "Epoch 300, Loss: 0.6597\n",
      "Epoch 400, Loss: 0.6597\n",
      "Epoch 500, Loss: 0.6597\n",
      "Epoch 600, Loss: 0.6597\n",
      "Epoch 700, Loss: 0.6597\n",
      "Epoch 800, Loss: 0.6597\n",
      "Epoch 900, Loss: 0.6597\n",
      "Backpropagation training time: 0.26 seconds\n",
      "Epoch 0, Loss: 0.6918\n",
      "Epoch 100, Loss: 0.6504\n",
      "Epoch 200, Loss: 0.6395\n",
      "Epoch 300, Loss: 0.6332\n",
      "Epoch 400, Loss: 0.5562\n",
      "Epoch 500, Loss: 0.5209\n",
      "Epoch 600, Loss: 0.5023\n",
      "Epoch 700, Loss: 0.4380\n",
      "Epoch 800, Loss: 0.4743\n",
      "Epoch 900, Loss: 0.3992\n",
      "Accuracy Classic-NN: 0.91\n",
      "\n",
      "Comparison Results:\n",
      "PSO-NN Accuracy: 0.99 | Time: 11.37 seconds\n",
      "Classic-NN Accuracy: 0.91 | Time: 0.26 seconds\n"
     ]
    }
   ],
   "source": [
    "import importlib\n",
    "import timeit\n",
    "\n",
    "# Import modules\n",
    "PSO = importlib.import_module(\"PSO-NN\")\n",
    "CS = importlib.import_module(\"classic-NN\")\n",
    "from commonsetup import n_hidden, X_train, X_test, y_train, y_test, n_inputs, n_classes, activation, n_iteration, learning_rate\n",
    "\n",
    "# Set PSO parameters\n",
    "par_C1 = 2.0\n",
    "par_C2 = 2.0\n",
    "par_W = 0.7\n",
    "par_SwarmSize = 100\n",
    "batchsize = 200  # Number of data instances used by the fitness function\n",
    "\n",
    "# Print settings\n",
    "print(\"############ You are using the following settings:\")\n",
    "print(\"Number of hidden layers:\", n_hidden)\n",
    "print(\"Activation function:\", activation[0])\n",
    "print(\"Number of variables to optimize:\", (n_inputs * n_hidden) + (n_hidden * n_classes) + n_hidden + n_classes)\n",
    "print(\"PSO parameters C1:\", par_C1, \"C2:\", par_C2, \"W:\", par_W, \"SwarmSize:\", par_SwarmSize, \"Iterations:\", n_iteration)\n",
    "print(\"\\n\")\n",
    "\n",
    "nn = PSO.NeuralNetwork(n_inputs, n_hidden, n_classes, activation[0])\n",
    "# Define function for PSO optimization\n",
    "def optimize_pso():\n",
    "    \n",
    "    pso = PSO.PSOOptimizer(nn, par_C1, par_C2, par_W, par_SwarmSize, n_iteration, batchsize)\n",
    "    weights = pso.optimize(X_train, y_train)\n",
    "    return weights\n",
    "\n",
    "# Measure time for PSO optimization\n",
    "pso_time = timeit.timeit(optimize_pso, number=1)\n",
    "print(f\"PSO optimization time: {pso_time:.2f} seconds\")\n",
    "\n",
    "# Evaluate PSO-NN accuracy\n",
    "weights = optimize_pso()\n",
    "y_pred = nn.predict(weights, X_test)\n",
    "pso_accuracy = (y_pred == y_test).mean()\n",
    "print(f\"Accuracy PSO-NN: {pso_accuracy:.2f}\")\n",
    "\n",
    "# Define function for backpropagation training\n",
    "def train_backprop():\n",
    "    nn = CS.NeuralNetwork(n_inputs, n_hidden, n_classes, activation[0], activation[1])\n",
    "    nn.train(X_train, y_train, n_iteration, learning_rate)\n",
    "    return nn\n",
    "\n",
    "# Measure time for backpropagation training\n",
    "backprop_time = timeit.timeit(train_backprop, number=1)\n",
    "print(f\"Backpropagation training time: {backprop_time:.2f} seconds\")\n",
    "\n",
    "# Evaluate Classic-NN accuracy\n",
    "nn = train_backprop()\n",
    "y_pred = nn.predict(X_test)\n",
    "backprop_accuracy = (y_pred == y_test).mean()\n",
    "print(f\"Accuracy Classic-NN: {backprop_accuracy:.2f}\")\n",
    "\n",
    "# Compare results\n",
    "print(\"\\nComparison Results:\")\n",
    "print(f\"PSO-NN Accuracy: {pso_accuracy:.2f} | Time: {pso_time:.2f} seconds\")\n",
    "print(f\"Classic-NN Accuracy: {backprop_accuracy:.2f} | Time: {backprop_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classical NN is way faster but has a worse accuracy than PSO-NN. PSO-NN is slower but has a better accuracy."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_assignments",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
